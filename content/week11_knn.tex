\section{K-Nearest-Neighbours (KNN)}
\subsection{Linearly spearable data}
\includegraphics[width=\linewidth]{linear-separability.png}
\begin{itemize}
  \item Based on logistic regression model, you can draw a line
  \item This is the Linear decision boundary
  \item If a simple line perfectly seperates the classes, then the classes are said to be linearely seperable
\end{itemize}
\subsection{Decision Boundary}
Boundary with two \textbf{features} (bsc grades and workex):\\
\includegraphics[width=\linewidth]{decision-boundary.png}\\
Use the logistic regression (sigmoid) for calculation of boundary.
If a simple line perfectly separates the classes, then the classes are said to be linearly separable!

\subsection{Multi-class classification}
LR can be extended to multiple classes in some obvious ways.

\subsubsection{One-vs.-rest}
\begin{itemize}
  \item train a single classifier for each class ``c'', with the samples of class ``c'' as positive samples and all other samples as negatives.
  \item Apply all classifiers to an unseen sample x
  \item the classifier reports the highest p is the class
\end{itemize}

\subsubsection{One-vs.-one}
\begin{itemize}
  \item Train classifiers to distinguish between each pair of classes (c1,c2), (c1, c3), (c1,c4), (c2, c3), (c2, c4), (c3,c4).
  \item Apply all classifiers to an unseen sample x.
  \item Combine the results to produce final classification.
\end{itemize}

\subsection{KNN classification}
A datapoint is known by the company it keeps.\\
Given a test data point, KNN:
\begin{itemize}
  \item computes k nearest neighbours of it,
  \item returns the most frequent class of the $k$ neighbours
\end{itemize}
For this, \textbf{all} distances to all points have to be calculated.
\includegraphics[width=\linewidth]{knn.png}

\subsection{KNN Details}
\begin{enumerate}
  \item Load the training as well as test data.
  \item Choose the value of $k$
  \item For each test data points $x_{test}$
  \begin{enumerate}
    \item For all training data $x_{train}$, calculate the $d(x_{test},x_{train})$
    \item Sort training data in the ascending order of the distance
    \item Choose the first $k$ data points from the sorted training data
    \item Choose the most frequently occurring class from the $k$ data points as the classification result
  \end{enumerate}
\end{enumerate}

\subsubsection{Advantages}
\begin{itemize}
    \item Easy and simple ML model
    \item Few hyperparameters to tune
\end{itemize}

\subsubsection{Disadvantages}
\begin{itemize}
    \item $k$ should be wisely selected
    \item Large computation cost during runtime if sample size is large
    \item Not efficient for high dimensional datasets
    \item Proper scaling should be provided for fair treatment among features
\end{itemize}

\subsubsection{Hyperparameters}
\begin{itemize}
    \item \textbf{K Value}: how many neighbours to participate in the KNN algo.
    \item \textbf{Distance Function}: Euclidean distance is most used
\end{itemize}

\subsubsection{Distance Metric}
\textbf{Cosine Distance}: See Week 2\\
\textbf{Manhattan Distance}: $d_M = \displaystyle\sum_{i = 1}^{n}|X_{1,n} - X_{2,n}|$\\
\includegraphics[width=\linewidth]{manhatten-distance.png}\\
\textbf{Euclidean Distance}: $d_E = \sqrt{\displaystyle\sum_{i = 1}^{n}(X_{1,n} - X_{2,n})^2}$\\
\includegraphics[width=\linewidth]{euclidean-distance.png}\\
\textbf{Minkowski Distance}:  generalization of both the Euclidean distance and the Manhattan distance
